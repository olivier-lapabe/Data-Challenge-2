{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import h5py\n","import numpy as np\n","import torch\n","from torchvision import transforms\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["file_path = './images_dataset2.hdf5'  # Update this to your HDF5 file path\n","hdf5_file = h5py.File(file_path, 'r')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def count_images(group):\n","    return len(group.keys())\n","\n","num_train_images = count_images(hdf5_file['train'])\n","num_val_images = count_images(hdf5_file['val'])\n","\n","print(f\"Number of training images: {num_train_images}\")\n","print(f\"Number of validation images: {num_val_images}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def check_normalization(group, num_samples=100):\n","    sample_keys = list(group.keys())[:num_samples]\n","    data = np.stack([group[key][()] for key in sample_keys])\n","    mean = np.mean(data)\n","    std = np.std(data)\n","    return mean, std\n","\n","train_mean, train_std = check_normalization(hdf5_file['train'])\n","val_mean, val_std = check_normalization(hdf5_file['val'])\n","\n","print(f\"Training data mean: {train_mean}, std: {train_std}\")\n","print(f\"Validation data mean: {val_mean}, std: {val_std}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def display_images_with_metadata(group, num_images=2):\n","    keys = list(group.keys())[:num_images]\n","    fig, axes = plt.subplots(2, num_images, figsize=(20, 8), gridspec_kw={'height_ratios': [3, 1]})\n","    for ax_img, ax_text, key in zip(axes[0], axes[1], keys):\n","        img_data = group[key][()]\n","        filename = group[key].attrs.get('filename', 'Unknown')\n","        full_path = group[key].attrs.get('full_path', 'Path not specified')\n","        gender = group[key].attrs.get('gender', 'Gender not specified')\n","        face_occlusion = group[key].attrs.get('FaceOcclusion', 'Occlusion not specified')\n","        \n","        # Afficher l'image\n","        ax_img.imshow(np.transpose(img_data, (1, 2, 0)))\n","        ax_img.axis('off')\n","        \n","        # Configurer le texte en dessous de l'image\n","        metadata_text = f\"File: {filename}\\nPath: {full_path}\\nGender: {gender}\\nOcclusion: {face_occlusion}\"\n","        ax_text.text(0.5, 0.5, metadata_text, ha='center', va='center', fontsize=10, wrap=True)\n","        ax_text.axis('off')\n","\n","    plt.tight_layout(pad=3.0)\n","    plt.show()\n","\n","display_images_with_metadata(hdf5_file['train'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hdf5_file.close()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":2}
