{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1def575",
   "metadata": {},
   "source": [
    "# Data Challenge - Advanced Machine Learning\n",
    "\n",
    "\n",
    "**The data challenge is proposed by Idemia**\n",
    "\n",
    "## Description of the data challenge\n",
    "\n",
    "You have at your disposal 100000 images of human faces, and their occlusion label.\n",
    "The goal of this challenge is to regress the percentage of the face that is occluded.\n",
    "We also want to have similar performances on female and male, the gender label is given for the train database.\n",
    "\n",
    "Below is the formula of the evaluation score. You'll first need to compute the error made by your model for men and women separetely. The error is measured as follows\n",
    "\n",
    "$$\n",
    " Err = \\frac{\\sum_{i}{w_i(p_i - GT_i)^2}}{\\sum_{i}{w_i}}, w_i = \\frac{1}{30} + GT_i,\n",
    "$$\n",
    "where $GT_i$ is the ground truth and p_i the prediction made by your model. \n",
    "\n",
    "Then, the final score for the leaderboard is given by\n",
    "\n",
    "$$\n",
    "Score = \\frac{Err_F + Err_M}{2} + \\left | Err_F - Err_M \\right |,\n",
    "$$\n",
    "where $Err_F$ and $Err_M$ are the errors obtained for female and men, respectively.\n",
    "\n",
    "Hereafter we show you how to load the data and run a naive baseline using a pretrained model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d771bd4",
   "metadata": {},
   "source": [
    "### Downloading the Data:\n",
    "\n",
    "You can download the Dataset from the below links: https://partage.imt.fr/index.php/s/mscQABX4oZxx7ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14a84fac-1388-46c0-8b2b-c5da6383b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries to run the naive baseline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import mobilenet_v3_small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb931c0-cd9d-4b23-8d96-1cfb9706c3de",
   "metadata": {},
   "source": [
    "### Load dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e8dbe42-0b3e-4649-8aa8-2c2ce3b8d7d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'listes_training/data_100K/train_100K.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlistes_training/data_100K/train_100K.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlistes_training/data_100K/test_students.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m image_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrops_100K\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documents/MS IA Telecom Paris/P3/BGDIA704 - Advanced Machine Learning/98_Data_challenge/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MS IA Telecom Paris/P3/BGDIA704 - Advanced Machine Learning/98_Data_challenge/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Documents/MS IA Telecom Paris/P3/BGDIA704 - Advanced Machine Learning/98_Data_challenge/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MS IA Telecom Paris/P3/BGDIA704 - Advanced Machine Learning/98_Data_challenge/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Documents/MS IA Telecom Paris/P3/BGDIA704 - Advanced Machine Learning/98_Data_challenge/.venv/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'listes_training/data_100K/train_100K.csv'"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"data/listes_training/data_100K/train_100K.csv\", delimiter=' ')\n",
    "df_test = pd.read_csv(\"data/listes_training/data_100K/test_students.csv\", delimiter=' ')\n",
    "\n",
    "image_dir = \"data/crops_100K\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b7e12a-f0f1-4779-9205-f4b7d44b84a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b973ad-10c6-44fd-bc23-3fbb644f12d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2225ac6-a74d-43f7-a5a2-dae2a8bd5728",
   "metadata": {},
   "source": [
    "#### Remove nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaef2a2-4512-460b-af97-f91cece13caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.dropna()\n",
    "df_test = df_test.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6835d8-2964-4822-bd1c-e15fba297e42",
   "metadata": {},
   "source": [
    "### Split Dataframe in train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34261442-22d9-4611-9a65-1ccc5fbc4bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df_train.loc[:20000].reset_index()\n",
    "df_train = df_train.loc[20000:].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309660aa-d8a9-404a-b7d7-5f4bb5dac72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train), len(df_val), len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925365d9-57c3-40e6-afe6-f555c9ebaa7c",
   "metadata": {},
   "source": [
    "### Check that all images are read correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13e82aa-db54-4961-b391-ea7d4721a377",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in tqdm(df_train.iterrows(), total=len(df_train)):\n",
    "    try:\n",
    "        filename = df_train.loc[idx, 'filename']\n",
    "        img2display = Image.open(f\"{image_dir}/{filename}\")\n",
    "    except ValueError as e:\n",
    "        print(idx, e)\n",
    "        \n",
    "for idx, row in tqdm(df_test.iterrows(), total=len(df_test)):\n",
    "    try:\n",
    "        filename = df_test.loc[idx, 'filename']\n",
    "        img2display = Image.open(f\"{image_dir}/{filename}\")\n",
    "    except ValueError as e:\n",
    "        print(idx, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f4aaa-05d3-4c47-a01e-415781e4ba72",
   "metadata": {},
   "source": [
    "### Display first images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6518d04-f6e5-4d5c-a51e-b34711825f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df_train[:5].iterrows():\n",
    "    filename = row['filename']\n",
    "    occlusion = row['FaceOcclusion']\n",
    "    gender = row['gender']\n",
    "    img2display = Image.open(f\"{image_dir}/{filename}\")\n",
    "    display(img2display)\n",
    "    print(filename, occlusion, gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1465fe98-f578-4d97-bae1-b1fcaa59183c",
   "metadata": {},
   "source": [
    "### Make Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b580142f-57ae-4a85-9185-340871f46581",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, df, image_dir):\n",
    "         'Initialization'\n",
    "         self.image_dir = image_dir\n",
    "         self.df = df\n",
    "         self.transform = transforms.ToTensor()\n",
    "         \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        row = self.df.loc[index]\n",
    "        filename = row['filename']\n",
    "\n",
    "        # Load data and get label\n",
    "        img = Image.open(f\"{image_dir}/{filename}\")\n",
    "        y = row['FaceOcclusion']\n",
    "        gender = row['gender']\n",
    "\n",
    "        X = self.transform(img)\n",
    "        y = np.float32(y)\n",
    "\n",
    "        return X, y, gender, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f9f263-4ae8-4922-b34a-4ce26e80c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = Dataset(df_train, image_dir)\n",
    "validation_set = Dataset(df_val, image_dir)\n",
    "\n",
    "params_train = {'batch_size': 8,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "\n",
    "params_val = {'batch_size': 8,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 0}\n",
    "\n",
    "training_generator = torch.utils.data.DataLoader(training_set, **params_train)\n",
    "validation_generator = torch.utils.data.DataLoader(validation_set, **params_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660e6c36-32e9-4f49-a655-7a4ff474f9df",
   "metadata": {},
   "source": [
    "### Create naive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c87728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPS for PyTorch\n",
    "use_mps = torch.backends.mps.is_available()\n",
    "device = torch.device(\"mps\" if use_mps else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1400b5-5c75-4512-9b55-3a49b7de99e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.mobilenet_v3_small(num_classes=1)\n",
    "model = model.to(device)   \n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c518a3-da5d-4ab4-aafe-17e4a4bbbf0a",
   "metadata": {},
   "source": [
    "### Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6c197d-1b93-405f-a2b3-ead6027e33f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7c0bc4-5415-4121-b22d-cba57ff7fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c317a740-179b-4056-a9c9-fee96bd151c7",
   "metadata": {},
   "source": [
    "### Train naive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf12e7c-c57f-4cac-bcc2-d244404a7d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.mps.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa6829d-8085-4e1a-b3d5-b24be1be83d4",
   "metadata": {},
   "source": [
    "#### Fit on train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38bfdf0-4dad-42c2-9d28-3cea32e07f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    print(f\"Epoch {n}\")\n",
    "    for batch_idx, (X, y, gender, filename) in tqdm(enumerate(training_generator), total=len(training_generator)):\n",
    "        # Transfer to GPU\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y = torch.reshape(y, (len(y), 1))\n",
    "        y_pred = model(X)\n",
    "        y_pred = y_pred.to(device)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        if loss.isnan():\n",
    "            print(filename)\n",
    "            print('label', y)\n",
    "            print('y_pred', y_pred)\n",
    "            break\n",
    "\n",
    "        if batch_idx % 200 == 0:\n",
    "            print(loss)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d57a8f-eb3b-4e51-9742-e551632a5e1b",
   "metadata": {},
   "source": [
    "## Evaluate metric on validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cb13ef-c632-4218-81d1-4044398bcd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_fn(df):\n",
    "    pred = df.loc[:, \"pred\"]\n",
    "    ground_truth = df.loc[:, \"target\"]\n",
    "    weight = 1/30 + ground_truth\n",
    "\n",
    "    return np.sum(((pred - ground_truth)**2) * weight, axis=0) / np.sum(weight, axis=0)\n",
    "\n",
    "def metric_fn(female, male):\n",
    "    err_male = error_fn(male)\n",
    "    err_female = error_fn(female)\n",
    "    return (err_male + err_female) / 2 + abs(err_male - err_female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d530c4-59ce-4a8b-9936-260c46ef5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "for batch_idx, (X, y, gender, filename) in tqdm(enumerate(validation_generator), total=len(validation_generator)):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model(X)\n",
    "        for i in range(len(X)):\n",
    "\n",
    "            results_list.append({'pred': float(y_pred[i]),\n",
    "                                  'target': float(y[i]),\n",
    "                                  'gender': float(gender[i])\n",
    "                                 })\n",
    "results_df = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a2ee7d-f0d6-46c3-96e8-8a6f11208b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd58f48-bd46-4d2d-a291-9b6af4e5e57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['pred'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063209ae-d673-4096-a8eb-487dd14b2ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_male = results_df.loc[results_df[\"gender\"] > 0.5]\n",
    "results_female = results_df.loc[results_df[\"gender\"] < 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8ebfae-8101-4b2c-bec5-9b60845f453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_fn(results_male, results_female)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4006c9fd",
   "metadata": {},
   "source": [
    "## Evaluating your Test DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5edd7ce-e687-433c-8248-0dd4d7a6f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_test(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, df, image_dir):\n",
    "         'Initialization'\n",
    "         self.image_dir = image_dir\n",
    "         self.df = df\n",
    "         self.transform = transforms.ToTensor()\n",
    "         \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        row = self.df.loc[index]\n",
    "        filename = row['filename']\n",
    "\n",
    "        # Load data and get label\n",
    "        img = Image.open(f\"{image_dir}/{filename}\")\n",
    "\n",
    "        X = self.transform(img)\n",
    "\n",
    "        return X, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cfdfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = Dataset_test(df_test, image_dir)\n",
    "\n",
    "params_test = {'batch_size': 8,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 0}\n",
    "\n",
    "test_generator = torch.utils.data.DataLoader(test_set, **params_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af5bcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "for batch_idx, (X, filename) in tqdm(enumerate(test_generator), total=len(test_generator)):\n",
    "        X = X.to(device)\n",
    "        y_pred = model(X)\n",
    "        for i in range(len(X)):\n",
    "\n",
    "            results_list.append({'pred': float(y_pred[i])\n",
    "                                 })\n",
    "test_df = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dbca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a67de71",
   "metadata": {},
   "source": [
    "### Preparing your file for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d8c3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"Data_Challenge.csv\", header = None, index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ff8155",
   "metadata": {},
   "source": [
    "#### Now it's your turn. Good luck !  :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
